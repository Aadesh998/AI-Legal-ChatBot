{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11385472,"sourceType":"datasetVersion","datasetId":7129322}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Installing the Library","metadata":{}},{"cell_type":"code","source":"!pip install -q groq\n!pip install -q pymupdf\n!pip install -q pillow\n!pip install -q torch\n!pip install -q torchvision\n!pip install -q transformers\n!pip install -q sentencepiece\n!pip install -q accelerate\n!pip install -q bitsandbytes\n!pip install -q faiss-cpu\n!pip install -q sentence-transformers","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T10:47:26.158003Z","iopub.execute_input":"2025-04-13T10:47:26.158221Z","iopub.status.idle":"2025-04-13T10:49:20.664499Z","shell.execute_reply.started":"2025-04-13T10:47:26.158194Z","shell.execute_reply":"2025-04-13T10:49:20.663663Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m126.7/126.7 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m20.0/20.0 MB\u001b[0m \u001b[31m83.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m82.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\npylibcugraph-cu12 24.12.0 requires pylibraft-cu12==24.12.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 24.12.0 requires rmm-cu12==24.12.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m24.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m30.7/30.7 MB\u001b[0m \u001b[31m64.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"# Importing necessary libraries","metadata":{}},{"cell_type":"code","source":"import os\nimport shutil\nimport fitz  # PyMuPDF for reading PDFs\nfrom PIL import Image  # for image processing\nimport numpy as np\n\nfrom groq import Groq  # Groq LLM client\nfrom transformers import AutoModel, AutoTokenizer  # For MiniCPM VLM model\nfrom sentence_transformers import SentenceTransformer  # Sentence embeddings\nimport faiss  # For semantic search","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T10:49:20.668945Z","iopub.execute_input":"2025-04-13T10:49:20.669609Z","iopub.status.idle":"2025-04-13T10:49:47.697074Z","shell.execute_reply.started":"2025-04-13T10:49:20.669577Z","shell.execute_reply":"2025-04-13T10:49:47.696322Z"}},"outputs":[{"name":"stderr","text":"2025-04-13 10:49:33.585945: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1744541373.778906      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1744541373.829907      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"# Initialize Groq LLM Client with your API key","metadata":{}},{"cell_type":"code","source":"api_key = \"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\"  # ğŸ” Replace with your Groq key\nclient = Groq(api_key=api_key)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T10:49:47.697923Z","iopub.execute_input":"2025-04-13T10:49:47.698422Z","iopub.status.idle":"2025-04-13T10:49:48.007939Z","shell.execute_reply.started":"2025-04-13T10:49:47.698402Z","shell.execute_reply":"2025-04-13T10:49:48.007332Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# Store chat history for coherent conversations\nhistory = []","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T10:50:59.843091Z","iopub.execute_input":"2025-04-13T10:50:59.843800Z","iopub.status.idle":"2025-04-13T10:50:59.847109Z","shell.execute_reply.started":"2025-04-13T10:50:59.843770Z","shell.execute_reply":"2025-04-13T10:50:59.846497Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"# Load the Vision-Language Model for OCR from images","metadata":{}},{"cell_type":"code","source":"vlm_model = AutoModel.from_pretrained('openbmb/MiniCPM-Llama3-V-2_5-int4', trust_remote_code=True)\nvlm_tokenizer = AutoTokenizer.from_pretrained('openbmb/MiniCPM-Llama3-V-2_5-int4', trust_remote_code=True)\nvlm_model.eval()  # Set model to evaluation mode","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T10:51:00.424421Z","iopub.execute_input":"2025-04-13T10:51:00.425137Z","iopub.status.idle":"2025-04-13T10:51:44.558207Z","shell.execute_reply.started":"2025-04-13T10:51:00.425113Z","shell.execute_reply":"2025-04-13T10:51:44.557413Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.91k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"64bf126dfcb94f70848dabb51a16a0d1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"configuration_minicpm.py:   0%|          | 0.00/4.06k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a4002792902d491080f24ac9e1fd8dce"}},"metadata":{}},{"name":"stderr","text":"A new version of the following files was downloaded from https://huggingface.co/openbmb/MiniCPM-Llama3-V-2_5-int4:\n- configuration_minicpm.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"modeling_minicpmv.py:   0%|          | 0.00/25.0k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9d3def1e307d419a85ed715409b2a831"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"resampler.py:   0%|          | 0.00/5.49k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8f161ceeaf694b83b3ac2049a8d2adda"}},"metadata":{}},{"name":"stderr","text":"A new version of the following files was downloaded from https://huggingface.co/openbmb/MiniCPM-Llama3-V-2_5-int4:\n- resampler.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\nA new version of the following files was downloaded from https://huggingface.co/openbmb/MiniCPM-Llama3-V-2_5-int4:\n- modeling_minicpmv.py\n- resampler.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/244k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d638e647172d4cbea3946bfbb3f75dcc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3135163b73334b4baab3914835ef518e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/1.51G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0b46789fae6d43658b1a09dff9043c23"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/4.65G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"35c2f73395554d71a3539614f47a9b4b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"34cdbbde85794c979370f0cb2a8fc25b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/121 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"966b376dd58944c3a6c05fbb7c1fd64e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/50.9k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8668b047a0074a8b85d31fca0d584c31"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/9.08M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1c097210540442b18be7eb8bdd086bd7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/459 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c03af45e25d043f88b598861f52da218"}},"metadata":{}},{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"MiniCPMV(\n  (llm): LlamaForCausalLM(\n    (model): LlamaModel(\n      (embed_tokens): Embedding(128256, 4096)\n      (layers): ModuleList(\n        (0-31): 32 x LlamaDecoderLayer(\n          (self_attn): LlamaAttention(\n            (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n            (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n            (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n            (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n          )\n          (mlp): LlamaMLP(\n            (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n            (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n            (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n            (act_fn): SiLU()\n          )\n          (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n          (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n        )\n      )\n      (norm): LlamaRMSNorm((4096,), eps=1e-05)\n      (rotary_emb): LlamaRotaryEmbedding()\n    )\n    (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n  )\n  (vpm): Idefics2VisionTransformer(\n    (embeddings): Idefics2VisionEmbeddings(\n      (patch_embedding): Conv2d(3, 1152, kernel_size=(14, 14), stride=(14, 14), padding=valid)\n      (position_embedding): Embedding(4900, 1152)\n    )\n    (encoder): Idefics2Encoder(\n      (layers): ModuleList(\n        (0-26): 27 x Idefics2EncoderLayer(\n          (self_attn): Idefics2VisionAttention(\n            (k_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n            (v_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n            (q_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n            (out_proj): Linear(in_features=1152, out_features=1152, bias=True)\n          )\n          (layer_norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n          (mlp): Idefics2VisionMLP(\n            (activation_fn): PytorchGELUTanh()\n            (fc1): Linear4bit(in_features=1152, out_features=4304, bias=True)\n            (fc2): Linear4bit(in_features=4304, out_features=1152, bias=True)\n          )\n          (layer_norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n        )\n      )\n    )\n    (post_layernorm): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n  )\n  (resampler): Resampler(\n    (kv_proj): Linear(in_features=1152, out_features=4096, bias=False)\n    (attn): MultiheadAttention(\n      (out_proj): NonDynamicallyQuantizableLinear(in_features=4096, out_features=4096, bias=True)\n    )\n    (ln_q): LayerNorm((4096,), eps=1e-06, elementwise_affine=True)\n    (ln_kv): LayerNorm((4096,), eps=1e-06, elementwise_affine=True)\n    (ln_post): LayerNorm((4096,), eps=1e-06, elementwise_affine=True)\n  )\n)"},"metadata":{}}],"execution_count":5},{"cell_type":"markdown","source":"\n# Load a lightweight sentence embedding model","metadata":{}},{"cell_type":"code","source":"embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T10:51:44.559759Z","iopub.execute_input":"2025-04-13T10:51:44.559996Z","iopub.status.idle":"2025-04-13T10:51:49.321099Z","shell.execute_reply.started":"2025-04-13T10:51:44.559978Z","shell.execute_reply":"2025-04-13T10:51:49.320515Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"285ef685941942d2a7f7c10790f98169"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"829a3adb7ea2493f8a99c0210876e261"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/10.5k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7cf3212dd1ea4e39b86f9606cc752356"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bd6edd4c65c24f66922c2483f8719606"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2f03a5e34a7b4ee78c44af386976f351"}},"metadata":{}},{"name":"stderr","text":"Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ca0eea714ef84c29aa382ae6231a7075"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cd79e12e3ddd4bbc869c5e5a4677f29e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"867b9b59ef94456ca9803261453dc7f8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0f8ca1a336394c688c4a4339aa59248f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0beaf9c862c94ea982459f61b8fbf672"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7d6418fb053f42debde2ac411fd06872"}},"metadata":{}}],"execution_count":6},{"cell_type":"markdown","source":"# Function to perform general chat without PDF using Groq LLM","metadata":{}},{"cell_type":"code","source":"def ask_groq(user_message):\n    history.append({\"role\": \"user\", \"content\": user_message})  # Add user's message\n    response = client.chat.completions.create(  # Get LLM response\n        model=\"llama3-8b-8192\",\n        messages=history,\n        temperature=0.7,\n        max_tokens=512,\n    )\n    reply = response.choices[0].message.content  # Extract reply\n    history.append({\"role\": \"assistant\", \"content\": reply})  # Save reply in history\n    return reply","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T10:51:49.321989Z","iopub.execute_input":"2025-04-13T10:51:49.322486Z","iopub.status.idle":"2025-04-13T10:51:49.326930Z","shell.execute_reply.started":"2025-04-13T10:51:49.322459Z","shell.execute_reply":"2025-04-13T10:51:49.326154Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"# Convert PDF pages into image files","metadata":{}},{"cell_type":"code","source":"def pdf_to_images(pdf_path, output_folder=\"output_images\"):\n    pdf_document = fitz.open(pdf_path)  # Open PDF\n    if os.path.exists(output_folder):  # Clean output folder if exists\n        shutil.rmtree(output_folder)\n    os.makedirs(output_folder)  # Create fresh output folder\n\n    for i in range(len(pdf_document)):  # Loop through pages\n        page = pdf_document.load_page(i)  # Load each page\n        pix = page.get_pixmap()  # Convert to pixel map\n        img = Image.frombytes(\"RGB\", [pix.width, pix.height], pix.samples)  # Convert to PIL Image\n        img.save(os.path.join(output_folder, f\"page_{i + 1}.png\"))  # Save image\n\n    return len(pdf_document), output_folder  # Return total pages and path\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T10:51:49.328985Z","iopub.execute_input":"2025-04-13T10:51:49.329484Z","iopub.status.idle":"2025-04-13T10:51:51.997929Z","shell.execute_reply.started":"2025-04-13T10:51:49.329453Z","shell.execute_reply":"2025-04-13T10:51:51.997232Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"# Extract text from image using MiniCPM model","metadata":{}},{"cell_type":"code","source":"def handwriting_ocr_img(image_path):\n    img = Image.open(image_path).convert('RGB')  # Open and convert image\n    question = 'Extract the text from the image. just give only image text'\n    msgs = [{'role': 'user', 'content': question}]\n    system_role = 'You are an AI that extracts and transcribes text from images accurately.'\n\n    # Use MiniCPM to process image and extract text\n    res = vlm_model.chat(\n        image=img,\n        msgs=msgs,\n        tokenizer=vlm_tokenizer,\n        sampling=True,\n        temperature=0.7,\n        system_prompt=system_role\n    )\n\n    return ''.join(res)  # Return extracted text","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T10:51:51.998815Z","iopub.execute_input":"2025-04-13T10:51:51.999099Z","iopub.status.idle":"2025-04-13T10:51:52.014694Z","shell.execute_reply.started":"2025-04-13T10:51:51.999077Z","shell.execute_reply":"2025-04-13T10:51:52.014012Z"}},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"# Function to extract and combine all text from PDF","metadata":{}},{"cell_type":"code","source":"def extract_text_from_pdf(pdf_path):\n    _, image_dir = pdf_to_images(pdf_path)  # Convert PDF to images\n    full_text = \"\"\n    for img in sorted(os.listdir(image_dir)):  # Loop over each image\n        path = os.path.join(image_dir, img)\n        full_text += handwriting_ocr_img(path)  # Add extracted text\n    return full_text","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T10:51:52.015813Z","iopub.execute_input":"2025-04-13T10:51:52.016022Z","iopub.status.idle":"2025-04-13T10:51:52.028001Z","shell.execute_reply.started":"2025-04-13T10:51:52.016007Z","shell.execute_reply":"2025-04-13T10:51:52.027258Z"}},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":"# Split large text into smaller chunks for embedding","metadata":{}},{"cell_type":"code","source":"def split_text_into_chunks(text, chunk_size=500):\n    sentences = text.split('. ')  # Split into sentences\n    chunks, current = [], \"\"  # Init chunk list\n    for sentence in sentences:\n        if len(current) + len(sentence) < chunk_size:\n            current += sentence + \". \"  # Add to current chunk\n        else:\n            chunks.append(current.strip())  # Save current chunk\n            current = sentence + \". \"\n    if current:\n        chunks.append(current.strip())  # Add remaining chunk\n    return chunks","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T10:51:52.028845Z","iopub.execute_input":"2025-04-13T10:51:52.029160Z","iopub.status.idle":"2025-04-13T10:51:52.040623Z","shell.execute_reply.started":"2025-04-13T10:51:52.029143Z","shell.execute_reply":"2025-04-13T10:51:52.040085Z"}},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":"# Build FAISS index from embedded chunks","metadata":{}},{"cell_type":"code","source":"def build_faiss_index(text_chunks):\n    embeddings = embedding_model.encode(text_chunks)  # Embed chunks\n    dim = embeddings.shape[1]  # Dimensionality\n    index = faiss.IndexFlatL2(dim)  # Create FAISS index\n    index.add(np.array(embeddings))  # Add embeddings\n    return index, embeddings","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T10:51:52.041412Z","iopub.execute_input":"2025-04-13T10:51:52.041692Z","iopub.status.idle":"2025-04-13T10:51:52.057113Z","shell.execute_reply.started":"2025-04-13T10:51:52.041666Z","shell.execute_reply":"2025-04-13T10:51:52.056448Z"}},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":"# Search relevant chunks and ask Groq to generate answer","metadata":{}},{"cell_type":"code","source":"def search_faiss_and_ask(question, chunks, index):\n    q_embed = embedding_model.encode([question])  # Embed question\n    _, top_indices = index.search(np.array(q_embed), k=3)  # Search top 3\n    retrieved = \"\\n\".join([chunks[i] for i in top_indices[0]])  # Get top chunks\n\n    # Format prompt with retrieved context\n    context = f\"Answer the question based on the following:\\n{retrieved}\"\n    messages = [\n        {\"role\": \"system\", \"content\": \"You are a helpful AI that answers based on given context.\"},\n        {\"role\": \"user\", \"content\": f\"{context}\\n\\nQuestion: {question}\"}\n    ]\n\n    # Ask Groq to answer using retrieved context\n    response = client.chat.completions.create(\n        model=\"llama3-8b-8192\",\n        messages=messages,\n        temperature=0.7,\n        max_tokens=500,\n    )\n    return response.choices[0].message.content\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T10:51:52.057882Z","iopub.execute_input":"2025-04-13T10:51:52.058136Z","iopub.status.idle":"2025-04-13T10:51:52.072692Z","shell.execute_reply.started":"2025-04-13T10:51:52.058116Z","shell.execute_reply":"2025-04-13T10:51:52.072066Z"}},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":"# Function to summarize entire document","metadata":{}},{"cell_type":"code","source":"def summarize_large_legal_text(text):\n    from textwrap import dedent  # For formatting\n    chunk_size = 8000\n    all_summaries = []\n\n    # Break large text into chunks\n    for i in range(0, len(text), chunk_size):\n        sub_text = text[i:i + chunk_size]\n        prompt = dedent(f\"\"\"\n        You are a legal AI assistant tasked with summarizing a legal document provided as text. Please perform the following:\n\n        1. Summarize the entire document concisely in 3-5 sentences.\n        2. Extract important dates.\n        3. Identify individuals and organizations mentioned.\n\n        TEXT TO ANALYZE:\n        {sub_text}\n        \"\"\")\n\n        # Get Groq summary for each chunk\n        response = client.chat.completions.create(\n            model=\"llama3-8b-8192\",\n            messages=[{\"role\": \"user\", \"content\": prompt}],\n            temperature=0.7,\n            max_tokens=500,\n        )\n\n        all_summaries.append(response.choices[0].message.content)\n\n    return \"\\n\\n--- Summary Chunk ---\\n\\n\".join(all_summaries)  # Combine summaries\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T10:51:52.074419Z","iopub.execute_input":"2025-04-13T10:51:52.074873Z","iopub.status.idle":"2025-04-13T10:51:52.087065Z","shell.execute_reply.started":"2025-04-13T10:51:52.074856Z","shell.execute_reply":"2025-04-13T10:51:52.086258Z"}},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":"# ========== MAIN ENTRY POINT ==========","metadata":{}},{"cell_type":"code","source":"def main():\n    print(\"\\nğŸ§  Welcome to the AI Legal Chatbot ğŸ§ \")\n    while True:\n        # Show user menu\n        print(\"\\nChoose an option:\")\n        print(\"1. Chat without PDF\")\n        print(\"2. Chat with PDF\")\n        print(\"3. Exit\")\n\n        choice = input(\"Enter your choice (1/2/3): \")\n\n        # Match-case switch (Python 3.10+)\n        match choice:\n            case \"1\":\n                while True:\n                    user_input = input(\"\\nYou: \")  # Get user input\n                    if user_input.lower() in [\"exit\", \"quit\"]:\n                        break\n                    response = ask_groq(user_input)  # Get response from Groq\n                    print(\"AI:\", response)\n\n            case \"2\":\n                pdf_path = input(\"\\nEnter path to your legal PDF: \").strip()\n                if not os.path.isfile(pdf_path):  # Check if file exists\n                    print(\"Invalid file path. Please try again.\")\n                    continue\n\n                print(\"Processing document...\")\n                text = extract_text_from_pdf(pdf_path)  # Extract OCR text\n                chunks = split_text_into_chunks(text)  # Chunk the text\n                index, _ = build_faiss_index(chunks)  # Build search index\n\n                while True:\n                    # Show PDF submenu\n                    print(\"\\nOptions:\")\n                    print(\"a. Ask question from document\")\n                    print(\"b. Summarize document\")\n                    print(\"c. Back to main menu\")\n                    sub_choice = input(\"Enter your choice (a/b/c): \")\n\n                    if sub_choice == 'a':\n                        q = input(\"Your question: \")\n                        answer = search_faiss_and_ask(q, chunks, index)\n                        print(\"Answer:\", answer)\n\n                    elif sub_choice == 'b':\n                        summary = summarize_large_legal_text(text)\n                        print(summary)\n\n                    elif sub_choice == 'c':\n                        break  # Go back to main menu\n\n                    else:\n                        print(\"Invalid option. Try again.\")\n\n            case \"3\":\n                print(\"ğŸ‘‹ Exiting. Thank you!\")  # Exit program\n                break\n\n            case _:\n                print(\"Invalid choice. Please enter 1, 2, or 3.\")  # Handle invalid input\n\n# Run program only when executed directly\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T10:51:52.087889Z","iopub.execute_input":"2025-04-13T10:51:52.088175Z","iopub.status.idle":"2025-04-13T10:52:56.022725Z","shell.execute_reply.started":"2025-04-13T10:51:52.088160Z","shell.execute_reply":"2025-04-13T10:52:56.022114Z"}},"outputs":[{"name":"stdout","text":"\nğŸ§  Welcome to the AI Legal Chatbot ğŸ§ \n\nChoose an option:\n1. Chat without PDF\n2. Chat with PDF\n3. Exit\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Enter your choice (1/2/3):  1\n\nYou:  hello\n"},{"name":"stdout","text":"AI: Hello! It's nice to meet you. Is there something I can help you with, or would you like to chat?\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"\nYou:  let me about the indian law\n"},{"name":"stdout","text":"AI: Indian law, also known as the law of India, refers to the legal system and laws of the Republic of India. The Indian legal system is based on the common law system and the Indian Constitution, which was adopted on January 26, 1950. Here are some key aspects of Indian law:\n\n**Constitution**: The Indian Constitution is the supreme law of the land and establishes the framework of the Indian government. It consists of 395 articles and 12 schedules.\n\n**Sources of Law**: Indian law derives from the following sources:\n\n1. The Constitution\n2. Statutes (Acts of Parliament)\n3. Common Law (judge-made law)\n4. Customary Law (traditional practices and customs)\n5. International Law (treaties and conventions)\n\n**Types of Laws**: Indian law encompasses various types of laws, including:\n\n1. **Criminal Law**: Deals with crimes and punishments, such as the Indian Penal Code.\n2. **Civil Law**: Deals with disputes between individuals, such as contract law and tort law.\n3. **Family Law**: Deals with family relationships, such as marriage, divorce, and inheritance.\n4. **Company Law**: Deals with corporate governance and regulation.\n5. **Tax Law**: Deals with taxation and revenue collection.\n\n**Key Areas of Law**: Some key areas of Indian law include:\n\n1. **Environmental Law**: Deals with environmental protection and conservation.\n2. **Intellectual Property Law**: Deals with patents, trademarks, copyrights, and other forms of intellectual property.\n3. **Labour Law**: Deals with labor rights and employment regulations.\n4. **Land Law**: Deals with land ownership, property rights, and land use.\n5. **Cyber Law**: Deals with online activities, data protection, and cybersecurity.\n\n**Court System**: The Indian court system is composed of:\n\n1. **Supreme Court**: The highest court in the land, with final appellate jurisdiction.\n2. **High Courts**: Jurisdiction over a specific state or region.\n3. **District Courts**: Jurisdiction over a specific district or area.\n4. **Magistrate Courts**: Jurisdiction over smaller matters and criminal cases.\n\n**Legal Professionals**: The legal profession in India includes:\n\n1. **Advocates**: Lawyers who practice law and appear in court.\n2. **Solicitors**: Lawyers who specialize in corporate law and business transactions.\n3. **Judges**: Trained legal professionals who preside over cases and make judgments.\n\nThis is just a brief overview of Indian law. If you have specific questions or would like\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"\nYou:  exit\n"},{"name":"stdout","text":"\nChoose an option:\n1. Chat without PDF\n2. Chat with PDF\n3. Exit\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Enter your choice (1/2/3):  2\n\nEnter path to your legal PDF:  exit\n"},{"name":"stdout","text":"Invalid file path. Please try again.\n\nChoose an option:\n1. Chat without PDF\n2. Chat with PDF\n3. Exit\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Enter your choice (1/2/3):  3\n"},{"name":"stdout","text":"ğŸ‘‹ Exiting. Thank you!\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}